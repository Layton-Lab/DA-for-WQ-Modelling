# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J7bxCUtpGHCEwgT1_1m_lRL3c0rIo2t4
"""
import datetime
import os
import torch
from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint

import wandb
from torch import optim, nn, utils, Tensor

import pytorch_lightning as pl
from collections import Counter,deque
import numpy as np
from torch.utils.data import TensorDataset, DataLoader
from pytorch_lightning.loggers import WandbLogger
import glob
import pandas as pd
from os import listdir
from os.path import isfile, join
import matplotlib.pyplot as plt
import re


#Converts HTLP excel to csv
def xlsx_to_csv(xlsx):
    # df=pd.read_excel("water_data.xlsx", domain + "_samples").drop_duplicates()
    sheets = pd.ExcelFile(xlsx)
    for sheet in sheets.sheet_names:
        df=pd.read_excel(sheets, sheet, parse_dates=["DateTime"]).drop_duplicates()
        df.to_csv(os.path.join(raw_path,sheet+".csv"), index=False)
    # df = df.drop('Value [COND] Conductivity (umho)', axis=1)
def min_max(df, min, max):
    return (df - min) / (max - min)

def plot_hist(title, data, folder=None, show=False,save=True):
    plt.hist(data,bins=40)
    plt.title(title)
    if save:
        plt.savefig(os.path.join(folder, title+".png"))
    if show:
        plt.show()


def check_flow_corr(csvs):
    flow_corr=pd.DataFrame.from_dict({'name':[csv.replace("_samples.csv", '') for csv in csvs]})
    df = pd.read_csv(os.path.join(raw_path, csvs[0]))
    val_cols = [col for col in df.columns if 'Value' in col]
    dfs=[]
    flow_corr[val_cols[1:]]=np.nan
    for i, csv in enumerate(csvs):
        df=pd.read_csv(os.path.join(clean_path, csv))
        flow_corr.loc[df.index[i], val_cols[1:]]=df.loc[:,val_cols].corr().iloc[0,1:]
        dfs.append(df)
    torch.save(flow_corr, os.path.join(clean_path, "flow corr"))

#First make the full time series
def clean_csvs(csvs):
    dfs=[]
    filtered_csvs=[]
    lens=[]
    for csv in csvs:
        df=pd.read_csv(os.path.join(raw_path, csv), parse_dates=["DateTime"]).drop_duplicates()
        val_cols = [col for col in df.columns if 'Value' in col]
        if (df.loc[:,val_cols].isna().sum()==len(df)).sum()>0:
            continue
        filtered_csvs.append(csv)
        # cyclic_day = (df.DateTime.dt.minute + df.DateTime.dt.hour * 60) / 1440 * 2 * np.pi
        # df.insert(1,'cyclic_day_sin', np.sin(cyclic_day))
        # df.insert(1,'cyclic_day_cos', np.cos(cyclic_day))
        df.insert(1, "date", (df.DateTime - pd.to_timedelta(1, unit='s')).round("1d"))
        df.drop("DateTime", axis=1, inplace=True)
        df = df.groupby("date").mean(numeric_only=True).asfreq('D').reset_index()

        cyclic_year = df.date.dt.dayofyear / 365 * 2 * np.pi
        df.insert(1, 'cyclic_year_sin', np.sin(cyclic_year))
        df.insert(1,'cyclic_year_cos', np.cos(cyclic_year))
        df.insert(1, "abs_time", (df.date - datetime.datetime(2000, 1, 1)).dt.total_seconds() / 60)

        #There is no date (still DateTime)
        for val_col in val_cols:
            qual_col=val_col.replace("Value", "Qualifiers")
            if qual_col in df.columns:
                df[qual_col] = np.where(df[val_col].isna(), 0, 1)
            else:
                df.insert(df.columns.get_loc(val_col),
                          qual_col,
                          np.where(df[val_col].isna(), 0, 1))

        dfs.append(df)
        lens.append(len(df))

    all_df=pd.concat(dfs, axis=0).reset_index(drop=True)
    val_cols = [col for col in all_df.columns if 'Value' in col]
    val_cols_neg=val_cols  #no conductivity (was [:-1]
    val_cols_neg_min=all_df.loc[:, val_cols_neg].min()
    for val_col in val_cols_neg:
        plot_hist("global "+re.findall("(\[.*\])",val_col)[0],
                  all_df.loc[:, val_col],
                  os.path.join(clean_path, "graphs"), show=True)
    all_df.loc[:, val_cols_neg]=np.log10(np.sqrt(all_df.loc[:, val_cols_neg]-val_cols_neg_min)+0.1)
    for val_col in val_cols_neg:
        plot_hist("global "+re.findall("(\[.*\])",val_col)[0] +" log transformed",
                  all_df.loc[:, val_col],
                  os.path.join(clean_path, "graphs"), show=True)

    all_mean=all_df.loc[:, val_cols].mean()
    all_std=all_df.loc[:, val_cols].std()
    all_df.loc[:, val_cols] = (all_df.loc[:, val_cols] - all_mean) / all_std
    for val_col in val_cols_neg:
        plot_hist("global "+re.findall("(\[.*\])",val_col)[0] +" standardized",
                  all_df.loc[:, val_col],
                  os.path.join(clean_path, "graphs"), show=True)
    all_abs_time_min=all_df["abs_time"].min()
    all_abs_time_max=all_df["abs_time"].max()
    all_df["abs_time"] = min_max(all_df["abs_time"], all_abs_time_min, all_abs_time_max)

    all_date_min = all_df["date"].min()
    all_date_max = all_df["date"].max()
    all_df = all_df.drop('date', axis=1)

    counter=0
    for length, csv in zip(lens, filtered_csvs):
        df=all_df.iloc[counter: counter+length]
        torch.save({"mean": df.mean(),
                    "std": df.std(),
                    "na": df.loc[:, val_cols].isna().sum()
                    },
            os.path.join(clean_path, csv.replace(".csv", '') + "_stats"))
        df=df.interpolate("linear", limit_direction="both")

        df.to_csv(os.path.join(clean_path, "log"+csv), index=False)
        counter+=length


    torch.save({"val_cols_neg_min":val_cols_neg_min,
                "all_min": all_mean,
                "all_logged_mean": all_mean,
                "all_logged_std": all_std,
                "all_abs_time_min": all_abs_time_min,
                "all_abs_time_max": all_abs_time_max,
                "all_dt_min":all_date_min,
                "all_dt_max":all_date_max},
    os.path.join(clean_path,"global_stats"))

#Second, create the 128-day windows
def make_water_data(domains, res=1):
    data=[]
    qual_cols = [col for col in pd.read_csv(os.path.join(clean_path, domains[0])).columns if 'Qual' in col]

    for domain in domains:
        df= pd.read_csv(os.path.join(clean_path, domain))
        data.append(pd.Series({"name": domain.replace("_samples.csv", ''),
                               "length": len(df),
                               "Average Nan Proportion": df.loc[:, qual_cols].mean().mean().round(2),
                               "Std Nan Proportion": round(df.loc[:, qual_cols].mean().std(),3)}))
        temp = np.stack([df[i:i + ex_window_length]
                        for i in range(0, len(df) - ex_window_length, window_distance)])
        torch.save(torch.tensor(temp).float(),
                   os.path.join(processed_path, domain.replace(".csv",'')))
    info={"domains":domains,
          "ex_predict_length" : ex_predict_length,
            "ex_warmup_length" : ex_warmup_length,
          "window_distance":window_distance,
          "data": pd.concat(data, axis=1).T}
    torch.save(info,
               os.path.join(processed_path, "info"))
#Finally make the data loaders
def make_data_loader(domains, pretrain=True, name=''):
    tensors=[torch.load(os.path.join(processed_path, domain+"_samples")) for domain in domains]
    lens=[tensor.shape[0] for tensor in tensors]
    split_fraction = 0.7
    if pretrain:
        train_tensors=perm(equalize_sizes([tensor[: int(split_fraction*len(tensor))] for tensor in tensors]))
        val_tensors=perm(equalize_sizes([tensor[int(split_fraction*len(tensor)):] for tensor in tensors]))
        train_dataloader = DataLoader(TensorDataset(*train_tensors), batch_size=32, shuffle=True)
        val_dataloader = DataLoader(TensorDataset(*val_tensors), batch_size=32)

        info = {"domains": domains,
                "ex_predict_length": ex_predict_length,
                "ex_warmup_length": ex_warmup_length,
                "window_distance": window_distance,
                "lens": lens,
                "split_fraction":split_fraction}
        torch.save({"info": info,
                    "val_dataloader": val_dataloader,
                    "train_dataloader":train_dataloader},
                   os.path.join(to_use_path, name))
    else:
        for split_fraction in [0.01,0.02,0.05,0.1,0.2,0.4,0.7]:
            train_tensors = equalize_sizes([tensor[: int(split_fraction * len(tensor))] for tensor in tensors])
            val_tensors = equalize_sizes([tensor[int(split_fraction * len(tensor)):] for tensor in tensors])
            train_dataloader = DataLoader(TensorDataset(*train_tensors), batch_size=32, shuffle=True)
            val_dataloader = DataLoader(TensorDataset(*val_tensors), batch_size=32)
            info = {"domains": domains,
                    "ex_predict_length": ex_predict_length,
                    "ex_warmup_length": ex_warmup_length,
                    "window_distance": window_distance,
                    "lens": lens,
                    "split_fraction":split_fraction}
            torch.save({"info": info,
                        "val_dataloader": val_dataloader,
                        "train_dataloader": train_dataloader},
                       os.path.join(to_use_path, f"{domains[0]} {split_fraction}"))
#Make target domain Data Loader from full time series
def make_water_data_res(domains, res_decrease):
    #skips directly to process since needs different tensors between train and val
    lens=[]
    folder=os.path.join(to_use_path, f"res_decrease {res_decrease}")
    split_fraction = 0.7

    train_tensors, val_tensors = [], []
    num_removed_count=Counter()
    num_removed=deque()
    if not os.path.exists(folder):
        os.makedirs(folder)
    for domain in domains:
        df= pd.read_csv(os.path.join(clean_path, domain))
        lens.append(len(df))
        cols = [col.replace('Value ','') for col in df.columns if 'Value' in col]
        cols.remove("[FLOW] Flow (cfs)")
        train_tensors_d, val_tensors_d= [], []
        for i in range(0, len(df) - ex_window_length, window_distance):
            train=i/(len(df) - ex_window_length)<split_fraction
            # train=(i/10-i//10)<split_fraction

            length=ex_window_length if train else ex_warmup_length
            df_res_dec=df[i:i+length].copy()
            for col in cols:
                val_col="Value "+ col
                qual_col=val_col.replace("Value", "Qualifiers")
                #reduce res
                num_removed.append((len(df_res_dec[df_res_dec[qual_col] == 1]) - int(length * (res_decrease)))/length)
                if len(df_res_dec[df_res_dec[qual_col]==1]) > int(length*(res_decrease)):
                    no_data_idx=df_res_dec[df_res_dec[qual_col]==1].sample(len(df_res_dec[df_res_dec[qual_col]==1])-
                                                               int(length*(res_decrease))).index
                    df_res_dec.loc[no_data_idx, qual_col] = 0
                    df_res_dec.loc[df_res_dec[qual_col] == 0, val_col] = pd.NA
                #increase res
                else:
                    pass

                    # data_idx=df_res_dec[df_res_dec[qual_col] == 0].sample(int(length * (res_decrease))-
                    #                                     len(df_res_dec[df_res_dec[qual_col] == 1])).index
                    # df_res_dec.loc[data_idx, qual_col] = 1

            df_res_dec=df_res_dec.interpolate("linear", limit_direction="both")
            if train:
                train_tensors_d.append(torch.tensor(df_res_dec.values))
            else:
                val_tensors_d.append(torch.tensor(
                    pd.concat((df_res_dec, df[i + ex_warmup_length: i + ex_window_length])).values))
        train_tensors.append(torch.stack(train_tensors_d).float())
        val_tensors.append(torch.stack(val_tensors_d).float())

    train_dataloader = DataLoader(TensorDataset(*perm(train_tensors)), batch_size=32, shuffle=True)
    val_dataloader = DataLoader(TensorDataset(*perm(val_tensors)), batch_size=32)
    num_removed_count=Counter(num_removed)

    info={"domains":domains,
          "ex_predict_length" : ex_predict_length,
            "ex_warmup_length" : ex_warmup_length,
          "window_distance":window_distance,
          "lens": lens,
          "res_decrease":res_decrease,
          "num_removed_count":num_removed_count}

    torch.save({"info": info,
                "val_dataloader": val_dataloader,
                "train_dataloader": train_dataloader},
               os.path.join(folder, f"Maumee no add perm"))

def equalize_sizes(tensors):
    max_len = max(tensors, key=lambda x: x.shape[0]).shape[0]
    tensors = [tensor.repeat(max_len //
                            tensor.shape[0] + 1, 1, 1)[:max_len]
                     for tensor in tensors]
    return tensors
def perm(tensors):
    return [tensor[torch.randperm(tensor.shape[0])] for tensor in tensors]
def numpy_it(t):
    return t.detach().cpu().numpy()



if __name__=="__main__":
    raw_path=os.path.join("Data","raw")
    clean_path=os.path.join("Data","cleaned", "log")
    # clean_path=os.path.join("Data","cleaned", )

    processed_path=os.path.join("Data","processed", "log")
    to_use_path=os.path.join("Data","to use", "log")

    #Make Time Series
    # xlsx_to_csv(os.path.join(raw_path, "collections", "water_data2.xlsx"))
    raw_csvs = [f for f in listdir(raw_path) if isfile(join(raw_path, f)) and f.endswith("csv")]
    # stats= [torch.load(os.path.join(clean_path,f))
    #         for f in listdir(clean_path) if isfile(join(clean_path, f)) and f.endswith("_stats")]
    # print("a")
    # torch.load("/home/cbcheung/Work/Thesis/Code/Real/Data/cleaned/Lost_samples_stats")
    # clean_csvs(raw_csvs)

    #Make Windows
    cleaned_csvs = [f for f in listdir(clean_path) if isfile(join(clean_path, f)) and f.endswith("csv")]
    check_flow_corr(cleaned_csvs)
    ex_predict_length = 64
    ex_warmup_length = 64
    ex_window_length = ex_warmup_length + ex_predict_length
    res_decrease = 1
    predict_length = ex_predict_length // res_decrease
    warmup_length = ex_warmup_length // res_decrease
    window_length = ex_window_length // res_decrease
    window_distance = 1

    #Make Data Loader
    make_water_data(cleaned_csvs)
    processed_data = [f for f in listdir(processed_path) if isfile(join(processed_path, f)) and f.endswith("samples")]

    #Make Target Domain Data Loaders
    # name="various"
    # raws = [
    #     "Maumee",
    #     "Portage",
    #     "Raisin",
    #     "Rock Creek",
    #     "S. Turkeyfoot",
    #     "Sandusky River",
    #     "Wolf",
    # ]
    # name = "Just Maumee"
    # raws = [
    #     "Maumee",
    #     # "Lost"
    # ]
    # name = "Just Cuyahoga"
    # raws = [
    #     "Cuyahoga",
    #     # "Lost"
    # ]
    # name = "Maumee Basin"
    raws = [
        "Maumee",
        "S. Turkeyfoot",
        "Blanchard River",
        "Potato Run",
        "Shallow Run",
        "Wolf",
        "Tiffin River",
        "West",
        # "Lost"
    ]
    # name = "Just Lost"
    # raws = [
    #     "Lost"
    # ]
    name = "pretrain 14"
    processed_data_names = [f.replace("_samples", '').replace("log", '') for f in processed_data]
    raws2=[]
    count=0
    for r in processed_data_names:
        if (count<6 or r in raws) and r!="Lost":
            raws2.append(r)
            if r not in raws:
                count+=1
    raws=raws2
    # #

    for i in [0.02,0.05,0.1,0.2,0.4]:
        make_water_data_res(["logLost_samples.csv"],i)


